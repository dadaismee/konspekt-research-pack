# ĞĞ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸  
(12.07.2025, 12:41:19)

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=2&annotation=RIX5H377) Â«Our simplified models of the free energy principle are scientific models. They in turn posit generative models, possessed by agents and employed by them to perform inference and action.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 2](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=2&annotation=A6MX9QPQ) Â«In each of our scientific models, the generative model in question takes the form of a joint probability distribution like p(w, x) or p(w, x, z) .Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 2](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=3&annotation=XV63XNSQ) Â«The inference problem addressed by the active inference framework concerns an agent who can observe data x and must infer the value of an unobservable state w . The unobservable state is assumed to cause observable data (Fig.Â 1). The agent is capable of harbouring beliefs about the unobservable state, and knows the statistical relationship between it and the observable data, which is represented as a joint probability distribution p(w, x).Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 3](zotero://select/library/items/SGFQ77UK)) $w$ is a value of an unobservable state and $x$ is observable data, where $w$ is assumed to cause $x$. An agent knows te statistical relationship between these in the form of $p(w, x)$

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=3&annotation=854F3BX8) Â«The table describes a joint probability distribution p(w, x), where w ranges over possible cat locations: w âˆˆ{kitchen, bedroom}, and x ranges over possible cat sounds: x âˆˆ{meow, purr}.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 3](zotero://select/library/items/SGFQ77UK)) $p(w, x)$ in free energy equations. $w$ and $x$ are sets of possible values

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=5&annotation=9QJRSQF4) Â«This learning task is relatively difficult. It should be distinguished from the simpler task of estimating w from an observation of x , which is called inferenceÂ» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 5](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=5&annotation=TVNDQB4L) Â«The formalism at the heart of active inference begins with the observation that it is sometimes impossible to follow Bayesian PrinciPle. In many of the situations in which statisticians would like to find p(w x) , the sumâˆ‘ w p(w, x) is computationally intractable so p(x) cannot be calculated. This usually happens when the state space is continuous rather than discrete, so the sum âˆ‘ becomes an integral âˆ« over an infinite number of points.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 5](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=5&annotation=LEKQ5A9L) Â«When this problem is formulated by statisticians, we usually begin with a set of possible distributions q, and search for the member of that set which lies as close to p(w x) as possible. We can do this indirectly by using a measure of inaccuracy. Active inference employs a measure of inaccuracy called variational free energy, labelled F. Because it is a measure of inaccuracy, smaller values are better than larger values. Given a set of candidate distributions q, the best is the one that produces the lowest value of F. Although the lowest possible value of F is given by the true posterior p(w x) , that might not be one of the available distributions q. In that case, the optimal q is the member of the set that yields the lowest value of F from among the available members.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 5](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=5&annotation=5C7JM6JU) Â«Variational free energy captures two sources of inaccuracy in belief and dictates how they ought to be traded off against one another. The two sources of inaccuracy are overfitting and failing to explain the data.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 5](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=5&annotation=S4M6WAT2) Â«Overfitting. According to lexico.com (2021), overfitting is â€œThe production of an analysis which corresponds too closely or exactly to a particular set of data, and may therefore fail to fit additional data or predict future observations reliably.â€Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 5](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=6&annotation=79EXZASJ) Â«You overfit when you choose a distribution q(w) that explains the current data very well, but fails to account for the wider range of statistical possibilities encapsulated by p(w)Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 6](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=6&annotation=H4P7BBGZ) Â«The cost of overfitting can therefore be measured by checking how far q(w) diverges from p(w) . The first term of F is a measure of this kind: This term, which is also called relative entropy or Kullback-Leibler divergence, measures how far a distribution q(w) differs from a distribution p(w).3 When q and p are identical, they coincide for every value of the sum. In this case the logarithm is always zero (because log a a = 0 ) so the total value of the sum is zero. As q and p get more and more different, the total value of the term increases. To avoid overfitting, q(w) should be close to p(w).Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 6](zotero://select/library/items/SGFQ77UK)) $\sum_w q(w) \log \frac{q(w)}{p(w)}$

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=6&annotation=2R36ZK3U) Â«Failing to explain the data. Mathematically, â€˜explaining the dataâ€™ means assigning high probability to events w that make the probability of x high. The penalty for failing to explain data is captured by the second term of F:Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 6](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=6&annotation=BTRZCB6C) Â«Variational free energy F is the sum of the penalties for overfitting and failing to explain the data:Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 6](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=6&annotation=T9EWBSFN) Â«w q(w) log q(w) p(w)Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 6](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=6&annotation=PRC3L7X9) Â«w q(w) log 1 p(x w)Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 6](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=6&annotation=D4XPGSIQ) Â«F(p, q, x)= w q(w) log q(w) p(w) Penalty for overfitting + w q(w) log 1 p(x w) Penalty for failing to explain the dataÂ» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 6](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=7&annotation=LGEBIC2P) Â«We set up the inference problem by saying that the agent knows the statistics p(w, x) , but might not have access to the marginal distribution p(x) . The agent was prohibited from following Bayesian PrinciPle for this reason. However, we did not address whether the agent has access to the prior p(w) or the likelihood p(x w). Since F includes both those terms, one would expect the agent needs them in order to use F to guide inference. As it turns out, the agent does not need access to the prior or the likelihood, because (4) simplifies to:Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 7](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=7&annotation=MFGVXGYC) Â«Given our assumptions so far, the agent has access to all three inputs to F in Eq. (5): â€¢ p: A joint distribution over w and x . The agentâ€™s generative model and, in this simple example, also the true general statistical connection between w and x. â€¢ q: A distribution over w . The agentâ€™s credences about the unobservable state, in light of observing a specific piece of data x. â€¢ x : A value of a random variable. The specific piece of data the agent has just observed.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 7](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=7&annotation=UR637T2F) Â«Free energy PrinciPle (inFerence): q(w) â† ï¿½ argmin q F Here argmin q means â€˜choose the distribution q that makes the following term as small as possibleâ€™.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 7](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=7&annotation=HEWSMEA5) Â«Notice that the form of Free energy PrinciPle (inFerence) is the same as that of Bayesian PrinciPle. In both cases you are told to perform a calculation and set q(w) equal to the resulting value. The difference is that Bayesian PrinciPle counsels a direct calculation via Bayesâ€™ theorem. In contrast, Free energy PrinciPle (inFerence) counsels what might be called an indirect calculation. You must assess candidate distributions q in order to find the one that produces the lowest value of F.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 7](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=7&annotation=DHWVDXGN) Â«F(p, q, x)= w q(w) log q(w) p(w, x)Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 7](zotero://select/library/items/SGFQ77UK)) Simplified version of the previous equation [[ĞœĞ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ° Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ° ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑĞ½ĞµÑ€Ğ³Ğ¸Ğ¸]]

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=7&annotation=T5ZPNWRD) Â«n the active inference literature, the penalty for overfitting is often labelled â€˜complexityâ€™. The penalty for failing to explain the data is usually presented as a reward for explaining the data well; it is therefore introduced as the negation of the term we use here, and is called â€˜accuracyâ€™. Consequently, variational free energy is defined as the difference between complexity and accuracy. The goal of inference is described as minimizing complexity while maximizing accuracy. Our presentation is mathematically equivalent.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 7](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=8&annotation=VHMZIC56) Â«In our cat example, p was given by the table of statistics of cat locations and noises, and we assumed the observer heard the cat meowing ( x = meow). To solve the cat problem using Free energy PrinciPle (inFerence) we could use one of the aforementioned algorithms, or simply test lots of different values of q(w) to see which one produces the lowest value of F in combination with these values of p and x.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 8](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=9&annotation=RI9CGPU8) Â«One of the potentially confusing aspects of active inference is that it treats the statistical model p as a measure of both probabilities and preferences at the same time. Later we will discuss possible justifications of this move; for now we assume it is interpretatively valid, in order to give as smooth an exposition as possible.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 9](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=9&annotation=Q4IZNWR9) Â«Recall that Free energy PrinciPle (inFerence) counsels choosing beliefs by minimising a function that measures the cost of inaccuracy. That function, F , is a sum of two kinds of penalty. Action selection is governed in the same way, but with a slightly different cost function called expected free energy and labelled G. The definition of G is closely related to that of F. The interpretation of the two penalty terms changes as the formalism is updated to reflect the fact we are now making measurements over expected future states. Since future states have yet to be observed, the agent must average over them to obtain expected values. The penalties are associated with failing to satisfy preferences and failing to minimize future surprise.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 9](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=9&annotation=QJAANPA5) Â«Failing to satisfy preferences. q(w z) is the assumed distribution over hidden states given our action. If we place the cat in the bedroom, where do we expect it to be? p(w) is now a preference distribution over hidden states. The first penalty term in G is a measure of how far the expected distribution of hidden states diverges from the preference distribution:Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 9](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=9&annotation=RWM5C25L) Â«w q(w z) log q(w z) p(w)Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 9](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=10&annotation=WBMDC728) Â«Failing to minimize future surprise. One of the tenets of active inference is that agents should act to ensure that future data are not too surprising. The second penalty term of G therefore measures how surprising future data would be, on average, if you performed z:Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 10](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=10&annotation=CKYTMBJZ) Â«Overall, expected free energy is a sum of these penalties:Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 10](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=10&annotation=VVDPH8M3) Â«Free energy PrinciPle (action): z â† ï¿½ argmin z GÂ» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 10](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=10&annotation=VQPR4SWF) Â«In the same sense that Free energy PrinciPle (inFerence) approximates Bayesian inference, it has been suggested that minimizing expected free energy can be reaÂ» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 10](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=10&annotation=J4DK8TDT) Â«w q(w z) x p(x w) log 1 p(x w)Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 10](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=10&annotation=VEQYHA9U) Â«G(p, q, z)= w q(w z) log q(w z) p(w) Penalty for failing to satisfy your preferences + w q(w z) x p(x w) log 1 p(x w) Penalty for failing to minimize expected surprise of future dataÂ» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 10](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=11&annotation=WBYMZQJT) Â«There are at least two reasons why this interpretation should be distinguished from utilities as decision theory traditionally understands them. First, you should not simply prefer that your cat always be the temperature that happens to occur most often according to the frequency distribution. Healthy functioning entails some fluctuation of temperatures throughout the day. The goal is not to maximise the value of this distribution, but to match future event frequencies to it. Second, preferences are just one consideration that must be taken into account when choosing actions. The preference penalty must be balanced against the surprise penalty.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 11](zotero://select/library/items/SGFQ77UK)) [[ĞœĞ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ° Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ° ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑĞ½ĞµÑ€Ğ³Ğ¸Ğ¸]]

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=12&annotation=M7XXRQ3Y) Â«One of the ways proponents of the framework turn this unusual interpretation to their advantage is by casting action as a form of inference: The mechanism underlying [minimizing expected free energy] is formally symmetric to perceptual inference, i.e., rather than inferring the cause of sensory data an organism must infer actions that best make sensory data accord with an internal representation of the environment. Buckley etÂ al. (2017: p.57), emphasis addedÂ» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 12](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=12&annotation=LI4XSBI5) Â«By starting with an inference problem in the form of expected free energy minimization, preferences emerge as the first term of Eq. (8). But attempting to achieve these preferences must be balanced against the second term, which explicitly counsels minimizing future surprise.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 12](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=13&annotation=7V5NXJYG) Â«(Pearl 1988). Roughly, in Pearlâ€™s sense the â€˜Markov blanketâ€™ of a focal node is the set of nodes that provide total information about the focal node. However, Markov blankets have taken on a special usage within active inference (Bruineberg etÂ al. 2021). In the sense required here, a Markov blanket can be understood as the set of nodes that â€˜screen offâ€™ the agent from nodes considered external to it.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 13](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=14&annotation=ES2JXETP) Â«The correspondence between high values of F and life-threatening states leads to a third form of the free energy principle: Free energy PrinciPle (selection): any system that survives long enough will act so as to appear to be minimizing F.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 14](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=14&annotation=W8EYYCDR) Â«In recent work Friston gives a deflationary interpretation on which agents do not in fact minimize anything, but perform acts which can be interpreted as minimizing F.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 14](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=15&annotation=QFTC55I6) Â«Free energy PrinciPle (selection) interprets p as a kind of fitness function in the form of a probability distribution over sensory states. When we measured the temperature of our cat, we obtained a frequency distribution that acted both as a description of what happened when the cat was previously healthy and as a prescription of what temperatures the cat should have if we want it to remain healthy. Free energy PrinciPle (selection) expands the scope of this basic idea, from cats to every biological system, and from temperature to every measurable property.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 15](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=16&annotation=22XBCL9N) Â«energy PrinciPle (selection) captures the rather banal point that systems can only ever occupy survivable states. If you are likely to be in states your successful ancestors were in, then you are likely to be successful. This trivial observation is reflected mathematically by the fact that variational free energy contains a reciprocal of p(w, x) : high values of p(w, x) therefore produce low values of F . Indeed, any function that contains this reciprocal (or its logarithm) as a component will be infinite when the probability is zero.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 16](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=16&annotation=BPTHXMC6) Â«It is easy to show that variational free energy is an upper bound on surprisal.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 16](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=16&annotation=RST29NQ2) Â«roof: rearranging (4) gives F = âˆ‘ w q(w) log q(w) p(wï¿½x) + log 1 p(x) . The first term is a relative entropy, which by Jensenâ€™s inequality is always greater than or equal to zero (Cover and Thomas 2006,Â p. 28). Therefore F â‰¥ log 1 p(x) .Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 16](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=17&annotation=92R2Z3CQ) Â«minimizing variational free energy is not the only way to minimize surprise. Any non-negative function added to surprisal is an upper bound on surprisal. Proponents need another premise that singles out variational free energy as the function organisms should be treated as minimizing.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 17](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=17&annotation=4DM8MTX9) Â«In our first model, p was a generative model employed by an agent. It was therefore interpreted as representing probabilities. 2. In our second model, in addition to representing probabilities, p measured the desirability of certain future states over others. It was therefore interpreted as representing preferences. 3. In our third model, p tallied the historical frequencies of a set of (hypothetical) ancestors. It was therefore interpreted as representing the fitness of different states.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 17](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=17&annotation=ZU7MWH6N) Â«Supporters of the framework often point to the third role to explain how p can simultaneously fulfil the first two. A historical tally of successful states denotes probabilities (i.e. ancestral frequencies) and preferences (i.e. future expected fitness).Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 17](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=17&annotation=BRRV8P4S) Â«Entities that employ representations to act successfully are distinct in important ways from entities that can be treated as if they employ representations as a consequence of the effects of physical laws.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 17](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=17&annotation=47FGBK98) Â«Sprevak (2020, Sect.Â 6.3) convincingly argues that Friston invokes two distinct senses of free energy, which here correspond roughly to roles 1 and 3. Sprevak cites Colombo and Wright (2018) as drawing a similar distinction. Williams (2021) distinguishes descriptive and explanatory versions of the free energy principle, seemingly tracking the same issue.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 17](zotero://select/library/items/SGFQ77UK)) > Friston uses the concept of free energy in at least two senses: probabilities and fitness function.

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=18&annotation=CTNZYSGW) Â«We also assumed that there was a single cause, w , of sensory data. Realistically, the external world is a panoply of criss-crossing causal paths. An adequate generative model would contain terms representing at least some of the interactions between unobservable states. Active inference captures these features by treating agents as employing hierarchical models of their external worlds. The first level of the hierarchy x is the sensory data, the second level w1 represents whatever causes sensory data, the third level w2 represents whatever causes w1 , and so on.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 18](zotero://select/library/items/SGFQ77UK)) > Hierarchical evolutionary model of convention formation? â€” [[@hawkins2021]] + https://probmods.org/

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=18&annotation=XEPUYPWN) Â«In principle, agents could be uncertain about any aspect of their representation of the world, so every model component can be subject to updating in light of evidence.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 18](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=18&annotation=7Z6C9BVZ) Â«Regarding action, instead of a single act z the framework enables decisions about sequences of acts. Such sequences are called policies and are usually labelled ğœ‹. Expected free energy can be calculated across an entire policy in order to determine which sequence of acts is optimal.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 18](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=20&annotation=CM85F96P) Â«most successful neural models were perhaps those spawned by the predictive processing tradition. Predictive processing was inspired by predictive coding, a technique in communications engineering (Elias 1955). In the 1980s and 1990s neuroscientists began investigating its plausibility as a model of visual perception (Kawato etÂ al. 1993; Rao and Ballard 1999; Srinivasan etÂ al. 1982). In the early 2000s, Friston (2002,Â p. 131) claimed that a predictive processing system could be constructed that performs variational inference (see also Friston 2003,Â pp. 1339â€“1340).Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 20](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=20&annotation=43FC5BJQ) Â«Very roughly, we can understand the relationship between these aspects in terms of Marrâ€™s hierarchy, which is usually said to have three levels: computational, algorithmic, and implementational (Marr 1982). In Fristonâ€™s scientific model of predictive processing, the computation is variational inference. The algorithm is the expectation-maximisation algorithm, a two-step process whereby two different mathematical operations are performed iteratively. Neal and Hinton (1998) had already shown that a version of that algorithm minimizes variational free energy. Friston claimed the algorithm could be implemented by the activities of (and structural relations between) individual neurons (for a simplified example see Bogacz 2017, Sect.Â 2â€“3).Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 20](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=23&annotation=ZE9S9GMM) Â«he claim that a small neural network is capable of minimizing variational free energy via encoding prediction error is verifiable by actually building such a network, as Bogacz (2017, Sect.Â 2â€“3) shows.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 23](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=30&annotation=KZ29JV65) Â«The FEP is a mathematical formulation that explains, from first principles, the characteristics of biological systems that are able to resist decay and persist over time. It rests on the idea that all biological systems instantiate a hierarchical generative model of the world that implicitly minimises its internal entropy by minimising free energy. Ramstead etÂ al. (2018,Â p. 2)Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 30](zotero://select/library/items/SGFQ77UK))

# ĞĞ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸  
(12.07.2025, 12:43:35)

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=2&annotation=RIX5H377) Â«Our simplified models of the free energy principle are scientific models. They in turn posit generative models, possessed by agents and employed by them to perform inference and action.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 2](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=2&annotation=A6MX9QPQ) Â«In each of our scientific models, the generative model in question takes the form of a joint probability distribution like p(w, x) or p(w, x, z) .Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 2](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=3&annotation=XV63XNSQ) Â«The inference problem addressed by the active inference framework concerns an agent who can observe data x and must infer the value of an unobservable state w . The unobservable state is assumed to cause observable data (Fig.Â 1). The agent is capable of harbouring beliefs about the unobservable state, and knows the statistical relationship between it and the observable data, which is represented as a joint probability distribution p(w, x).Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 3](zotero://select/library/items/SGFQ77UK)) $w$ is a value of an unobservable state and $x$ is observable data, where $w$ is assumed to cause $x$. An agent knows te statistical relationship between these in the form of $p(w, x)$

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=3&annotation=854F3BX8) Â«The table describes a joint probability distribution p(w, x), where w ranges over possible cat locations: w âˆˆ{kitchen, bedroom}, and x ranges over possible cat sounds: x âˆˆ{meow, purr}.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 3](zotero://select/library/items/SGFQ77UK)) $p(w, x)$ in free energy equations. $w$ and $x$ are sets of possible values

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=5&annotation=9QJRSQF4) Â«This learning task is relatively difficult. It should be distinguished from the simpler task of estimating w from an observation of x , which is called inferenceÂ» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 5](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=5&annotation=TVNDQB4L) Â«The formalism at the heart of active inference begins with the observation that it is sometimes impossible to follow Bayesian PrinciPle. In many of the situations in which statisticians would like to find p(w x) , the sumâˆ‘ w p(w, x) is computationally intractable so p(x) cannot be calculated. This usually happens when the state space is continuous rather than discrete, so the sum âˆ‘ becomes an integral âˆ« over an infinite number of points.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 5](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=5&annotation=LEKQ5A9L) Â«When this problem is formulated by statisticians, we usually begin with a set of possible distributions q, and search for the member of that set which lies as close to p(w x) as possible. We can do this indirectly by using a measure of inaccuracy. Active inference employs a measure of inaccuracy called variational free energy, labelled F. Because it is a measure of inaccuracy, smaller values are better than larger values. Given a set of candidate distributions q, the best is the one that produces the lowest value of F. Although the lowest possible value of F is given by the true posterior p(w x) , that might not be one of the available distributions q. In that case, the optimal q is the member of the set that yields the lowest value of F from among the available members.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 5](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=5&annotation=5C7JM6JU) Â«Variational free energy captures two sources of inaccuracy in belief and dictates how they ought to be traded off against one another. The two sources of inaccuracy are overfitting and failing to explain the data.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 5](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=5&annotation=S4M6WAT2) Â«Overfitting. According to lexico.com (2021), overfitting is â€œThe production of an analysis which corresponds too closely or exactly to a particular set of data, and may therefore fail to fit additional data or predict future observations reliably.â€Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 5](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=6&annotation=79EXZASJ) Â«You overfit when you choose a distribution q(w) that explains the current data very well, but fails to account for the wider range of statistical possibilities encapsulated by p(w)Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 6](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=6&annotation=H4P7BBGZ) Â«The cost of overfitting can therefore be measured by checking how far q(w) diverges from p(w) . The first term of F is a measure of this kind: This term, which is also called relative entropy or Kullback-Leibler divergence, measures how far a distribution q(w) differs from a distribution p(w).3 When q and p are identical, they coincide for every value of the sum. In this case the logarithm is always zero (because log a a = 0 ) so the total value of the sum is zero. As q and p get more and more different, the total value of the term increases. To avoid overfitting, q(w) should be close to p(w).Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 6](zotero://select/library/items/SGFQ77UK)) $\sum_w q(w) \log \frac{q(w)}{p(w)}$

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=6&annotation=2R36ZK3U) Â«Failing to explain the data. Mathematically, â€˜explaining the dataâ€™ means assigning high probability to events w that make the probability of x high. The penalty for failing to explain data is captured by the second term of F:Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 6](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=6&annotation=BTRZCB6C) Â«Variational free energy F is the sum of the penalties for overfitting and failing to explain the data:Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 6](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=6&annotation=T9EWBSFN) Â«w q(w) log q(w) p(w)Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 6](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=6&annotation=PRC3L7X9) Â«w q(w) log 1 p(x w)Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 6](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=6&annotation=D4XPGSIQ) Â«F(p, q, x)= w q(w) log q(w) p(w) Penalty for overfitting + w q(w) log 1 p(x w) Penalty for failing to explain the dataÂ» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 6](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=7&annotation=LGEBIC2P) Â«We set up the inference problem by saying that the agent knows the statistics p(w, x) , but might not have access to the marginal distribution p(x) . The agent was prohibited from following Bayesian PrinciPle for this reason. However, we did not address whether the agent has access to the prior p(w) or the likelihood p(x w). Since F includes both those terms, one would expect the agent needs them in order to use F to guide inference. As it turns out, the agent does not need access to the prior or the likelihood, because (4) simplifies to:Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 7](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=7&annotation=MFGVXGYC) Â«Given our assumptions so far, the agent has access to all three inputs to F in Eq. (5): â€¢ p: A joint distribution over w and x . The agentâ€™s generative model and, in this simple example, also the true general statistical connection between w and x. â€¢ q: A distribution over w . The agentâ€™s credences about the unobservable state, in light of observing a specific piece of data x. â€¢ x : A value of a random variable. The specific piece of data the agent has just observed.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 7](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=7&annotation=UR637T2F) Â«Free energy PrinciPle (inFerence): q(w) â† ï¿½ argmin q F Here argmin q means â€˜choose the distribution q that makes the following term as small as possibleâ€™.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 7](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=7&annotation=HEWSMEA5) Â«Notice that the form of Free energy PrinciPle (inFerence) is the same as that of Bayesian PrinciPle. In both cases you are told to perform a calculation and set q(w) equal to the resulting value. The difference is that Bayesian PrinciPle counsels a direct calculation via Bayesâ€™ theorem. In contrast, Free energy PrinciPle (inFerence) counsels what might be called an indirect calculation. You must assess candidate distributions q in order to find the one that produces the lowest value of F.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 7](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=7&annotation=DHWVDXGN) Â«F(p, q, x)= w q(w) log q(w) p(w, x)Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 7](zotero://select/library/items/SGFQ77UK)) Simplified version of the previous equation [[ĞœĞ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ° Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ° ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑĞ½ĞµÑ€Ğ³Ğ¸Ğ¸]]

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=7&annotation=T5ZPNWRD) Â«n the active inference literature, the penalty for overfitting is often labelled â€˜complexityâ€™. The penalty for failing to explain the data is usually presented as a reward for explaining the data well; it is therefore introduced as the negation of the term we use here, and is called â€˜accuracyâ€™. Consequently, variational free energy is defined as the difference between complexity and accuracy. The goal of inference is described as minimizing complexity while maximizing accuracy. Our presentation is mathematically equivalent.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 7](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=8&annotation=VHMZIC56) Â«In our cat example, p was given by the table of statistics of cat locations and noises, and we assumed the observer heard the cat meowing ( x = meow). To solve the cat problem using Free energy PrinciPle (inFerence) we could use one of the aforementioned algorithms, or simply test lots of different values of q(w) to see which one produces the lowest value of F in combination with these values of p and x.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 8](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=9&annotation=RI9CGPU8) Â«One of the potentially confusing aspects of active inference is that it treats the statistical model p as a measure of both probabilities and preferences at the same time. Later we will discuss possible justifications of this move; for now we assume it is interpretatively valid, in order to give as smooth an exposition as possible.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 9](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=9&annotation=Q4IZNWR9) Â«Recall that Free energy PrinciPle (inFerence) counsels choosing beliefs by minimising a function that measures the cost of inaccuracy. That function, F , is a sum of two kinds of penalty. Action selection is governed in the same way, but with a slightly different cost function called expected free energy and labelled G. The definition of G is closely related to that of F. The interpretation of the two penalty terms changes as the formalism is updated to reflect the fact we are now making measurements over expected future states. Since future states have yet to be observed, the agent must average over them to obtain expected values. The penalties are associated with failing to satisfy preferences and failing to minimize future surprise.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 9](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=9&annotation=QJAANPA5) Â«Failing to satisfy preferences. q(w z) is the assumed distribution over hidden states given our action. If we place the cat in the bedroom, where do we expect it to be? p(w) is now a preference distribution over hidden states. The first penalty term in G is a measure of how far the expected distribution of hidden states diverges from the preference distribution:Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 9](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=9&annotation=RWM5C25L) Â«w q(w z) log q(w z) p(w)Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 9](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=10&annotation=WBMDC728) Â«Failing to minimize future surprise. One of the tenets of active inference is that agents should act to ensure that future data are not too surprising. The second penalty term of G therefore measures how surprising future data would be, on average, if you performed z:Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 10](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=10&annotation=CKYTMBJZ) Â«Overall, expected free energy is a sum of these penalties:Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 10](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=10&annotation=VVDPH8M3) Â«Free energy PrinciPle (action): z â† ï¿½ argmin z GÂ» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 10](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=10&annotation=VQPR4SWF) Â«In the same sense that Free energy PrinciPle (inFerence) approximates Bayesian inference, it has been suggested that minimizing expected free energy can be reaÂ» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 10](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=10&annotation=J4DK8TDT) Â«w q(w z) x p(x w) log 1 p(x w)Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 10](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=10&annotation=VEQYHA9U) Â«G(p, q, z)= w q(w z) log q(w z) p(w) Penalty for failing to satisfy your preferences + w q(w z) x p(x w) log 1 p(x w) Penalty for failing to minimize expected surprise of future dataÂ» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 10](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=11&annotation=IEVTH9V4) Â«Finally, let us present a solution to the cat example. For the problem to have a determinate solution we need a conditional distribution q(w z). Letâ€™s suppose that if we put the cat in the kitchen it usually stays there, but if we put it in the bedroom it tends to wander:Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 11](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=11&annotation=WBYMZQJT) Â«There are at least two reasons why this interpretation should be distinguished from utilities as decision theory traditionally understands them. First, you should not simply prefer that your cat always be the temperature that happens to occur most often according to the frequency distribution. Healthy functioning entails some fluctuation of temperatures throughout the day. The goal is not to maximise the value of this distribution, but to match future event frequencies to it. Second, preferences are just one consideration that must be taken into account when choosing actions. The preference penalty must be balanced against the surprise penalty.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 11](zotero://select/library/items/SGFQ77UK)) [[ĞœĞ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ° Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ° ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑĞ½ĞµÑ€Ğ³Ğ¸Ğ¸]]

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=12&annotation=M7XXRQ3Y) Â«One of the ways proponents of the framework turn this unusual interpretation to their advantage is by casting action as a form of inference: The mechanism underlying [minimizing expected free energy] is formally symmetric to perceptual inference, i.e., rather than inferring the cause of sensory data an organism must infer actions that best make sensory data accord with an internal representation of the environment. Buckley etÂ al. (2017: p.57), emphasis addedÂ» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 12](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=12&annotation=LI4XSBI5) Â«By starting with an inference problem in the form of expected free energy minimization, preferences emerge as the first term of Eq. (8). But attempting to achieve these preferences must be balanced against the second term, which explicitly counsels minimizing future surprise.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 12](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=13&annotation=7V5NXJYG) Â«(Pearl 1988). Roughly, in Pearlâ€™s sense the â€˜Markov blanketâ€™ of a focal node is the set of nodes that provide total information about the focal node. However, Markov blankets have taken on a special usage within active inference (Bruineberg etÂ al. 2021). In the sense required here, a Markov blanket can be understood as the set of nodes that â€˜screen offâ€™ the agent from nodes considered external to it.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 13](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=14&annotation=ES2JXETP) Â«The correspondence between high values of F and life-threatening states leads to a third form of the free energy principle: Free energy PrinciPle (selection): any system that survives long enough will act so as to appear to be minimizing F.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 14](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=14&annotation=W8EYYCDR) Â«In recent work Friston gives a deflationary interpretation on which agents do not in fact minimize anything, but perform acts which can be interpreted as minimizing F.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 14](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=15&annotation=QFTC55I6) Â«Free energy PrinciPle (selection) interprets p as a kind of fitness function in the form of a probability distribution over sensory states. When we measured the temperature of our cat, we obtained a frequency distribution that acted both as a description of what happened when the cat was previously healthy and as a prescription of what temperatures the cat should have if we want it to remain healthy. Free energy PrinciPle (selection) expands the scope of this basic idea, from cats to every biological system, and from temperature to every measurable property.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 15](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=16&annotation=22XBCL9N) Â«energy PrinciPle (selection) captures the rather banal point that systems can only ever occupy survivable states. If you are likely to be in states your successful ancestors were in, then you are likely to be successful. This trivial observation is reflected mathematically by the fact that variational free energy contains a reciprocal of p(w, x) : high values of p(w, x) therefore produce low values of F . Indeed, any function that contains this reciprocal (or its logarithm) as a component will be infinite when the probability is zero.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 16](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=16&annotation=BPTHXMC6) Â«It is easy to show that variational free energy is an upper bound on surprisal.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 16](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=16&annotation=RST29NQ2) Â«roof: rearranging (4) gives F = âˆ‘ w q(w) log q(w) p(wï¿½x) + log 1 p(x) . The first term is a relative entropy, which by Jensenâ€™s inequality is always greater than or equal to zero (Cover and Thomas 2006,Â p. 28). Therefore F â‰¥ log 1 p(x) .Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 16](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=17&annotation=92R2Z3CQ) Â«minimizing variational free energy is not the only way to minimize surprise. Any non-negative function added to surprisal is an upper bound on surprisal. Proponents need another premise that singles out variational free energy as the function organisms should be treated as minimizing.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 17](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=17&annotation=4DM8MTX9) Â«In our first model, p was a generative model employed by an agent. It was therefore interpreted as representing probabilities. 2. In our second model, in addition to representing probabilities, p measured the desirability of certain future states over others. It was therefore interpreted as representing preferences. 3. In our third model, p tallied the historical frequencies of a set of (hypothetical) ancestors. It was therefore interpreted as representing the fitness of different states.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 17](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=17&annotation=ZU7MWH6N) Â«Supporters of the framework often point to the third role to explain how p can simultaneously fulfil the first two. A historical tally of successful states denotes probabilities (i.e. ancestral frequencies) and preferences (i.e. future expected fitness).Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 17](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=17&annotation=BRRV8P4S) Â«Entities that employ representations to act successfully are distinct in important ways from entities that can be treated as if they employ representations as a consequence of the effects of physical laws.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 17](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=17&annotation=47FGBK98) Â«Sprevak (2020, Sect.Â 6.3) convincingly argues that Friston invokes two distinct senses of free energy, which here correspond roughly to roles 1 and 3. Sprevak cites Colombo and Wright (2018) as drawing a similar distinction. Williams (2021) distinguishes descriptive and explanatory versions of the free energy principle, seemingly tracking the same issue.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 17](zotero://select/library/items/SGFQ77UK)) > Friston uses the concept of free energy in at least two senses: probabilities and fitness function.

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=18&annotation=CTNZYSGW) Â«We also assumed that there was a single cause, w , of sensory data. Realistically, the external world is a panoply of criss-crossing causal paths. An adequate generative model would contain terms representing at least some of the interactions between unobservable states. Active inference captures these features by treating agents as employing hierarchical models of their external worlds. The first level of the hierarchy x is the sensory data, the second level w1 represents whatever causes sensory data, the third level w2 represents whatever causes w1 , and so on.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 18](zotero://select/library/items/SGFQ77UK)) > Hierarchical evolutionary model of convention formation? â€” [[@hawkins2021]] + https://probmods.org/

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=18&annotation=XEPUYPWN) Â«In principle, agents could be uncertain about any aspect of their representation of the world, so every model component can be subject to updating in light of evidence.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 18](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=18&annotation=7Z6C9BVZ) Â«Regarding action, instead of a single act z the framework enables decisions about sequences of acts. Such sequences are called policies and are usually labelled ğœ‹. Expected free energy can be calculated across an entire policy in order to determine which sequence of acts is optimal.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 18](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=20&annotation=CM85F96P) Â«most successful neural models were perhaps those spawned by the predictive processing tradition. Predictive processing was inspired by predictive coding, a technique in communications engineering (Elias 1955). In the 1980s and 1990s neuroscientists began investigating its plausibility as a model of visual perception (Kawato etÂ al. 1993; Rao and Ballard 1999; Srinivasan etÂ al. 1982). In the early 2000s, Friston (2002,Â p. 131) claimed that a predictive processing system could be constructed that performs variational inference (see also Friston 2003,Â pp. 1339â€“1340).Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 20](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=20&annotation=43FC5BJQ) Â«Very roughly, we can understand the relationship between these aspects in terms of Marrâ€™s hierarchy, which is usually said to have three levels: computational, algorithmic, and implementational (Marr 1982). In Fristonâ€™s scientific model of predictive processing, the computation is variational inference. The algorithm is the expectation-maximisation algorithm, a two-step process whereby two different mathematical operations are performed iteratively. Neal and Hinton (1998) had already shown that a version of that algorithm minimizes variational free energy. Friston claimed the algorithm could be implemented by the activities of (and structural relations between) individual neurons (for a simplified example see Bogacz 2017, Sect.Â 2â€“3).Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 20](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=23&annotation=ZE9S9GMM) Â«he claim that a small neural network is capable of minimizing variational free energy via encoding prediction error is verifiable by actually building such a network, as Bogacz (2017, Sect.Â 2â€“3) shows.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 23](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=30&annotation=KZ29JV65) Â«The FEP is a mathematical formulation that explains, from first principles, the characteristics of biological systems that are able to resist decay and persist over time. It rests on the idea that all biological systems instantiate a hierarchical generative model of the world that implicitly minimises its internal entropy by minimising free energy. Ramstead etÂ al. (2018,Â p. 2)Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 30](zotero://select/library/items/SGFQ77UK))

# ĞĞ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸  
(19.08.2024, 12:12:07)

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=2&annotation=RIX5H377) Â«Our simplified models of the free energy principle are scientific models. They in turn posit generative models, possessed by agents and employed by them to perform inference and action.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 2](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=2&annotation=A6MX9QPQ) Â«In each of our scientific models, the generative model in question takes the form of a joint probability distribution like p(w, x) or p(w, x, z) .Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 2](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=3&annotation=XV63XNSQ) Â«The inference problem addressed by the active inference framework concerns an agent who can observe data x and must infer the value of an unobservable state w . The unobservable state is assumed to cause observable data (Fig.Â 1). The agent is capable of harbouring beliefs about the unobservable state, and knows the statistical relationship between it and the observable data, which is represented as a joint probability distribution p(w, x).Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 3](zotero://select/library/items/SGFQ77UK)) $w$ is a value of an unobservable state and $x$ is observable data, where $w$ is assumed to cause $x$. An agent knows te statistical relationship between these in the form of $p(w, x)$

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=3&annotation=854F3BX8) Â«The table describes a joint probability distribution p(w, x), where w ranges over possible cat locations: w âˆˆ{kitchen, bedroom}, and x ranges over possible cat sounds: x âˆˆ{meow, purr}.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 3](zotero://select/library/items/SGFQ77UK)) $p(w, x)$ in free energy equations. $w$ and $x$ are sets of possible values

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=5&annotation=9QJRSQF4) Â«This learning task is relatively difficult. It should be distinguished from the simpler task of estimating w from an observation of x , which is called inferenceÂ» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 5](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=5&annotation=TVNDQB4L) Â«The formalism at the heart of active inference begins with the observation that it is sometimes impossible to follow Bayesian PrinciPle. In many of the situations in which statisticians would like to find p(w x) , the sumâˆ‘ w p(w, x) is computationally intractable so p(x) cannot be calculated. This usually happens when the state space is continuous rather than discrete, so the sum âˆ‘ becomes an integral âˆ« over an infinite number of points.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 5](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=5&annotation=LEKQ5A9L) Â«When this problem is formulated by statisticians, we usually begin with a set of possible distributions q, and search for the member of that set which lies as close to p(w x) as possible. We can do this indirectly by using a measure of inaccuracy. Active inference employs a measure of inaccuracy called variational free energy, labelled F. Because it is a measure of inaccuracy, smaller values are better than larger values. Given a set of candidate distributions q, the best is the one that produces the lowest value of F. Although the lowest possible value of F is given by the true posterior p(w x) , that might not be one of the available distributions q. In that case, the optimal q is the member of the set that yields the lowest value of F from among the available members.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 5](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=5&annotation=5C7JM6JU) Â«Variational free energy captures two sources of inaccuracy in belief and dictates how they ought to be traded off against one another. The two sources of inaccuracy are overfitting and failing to explain the data.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 5](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=5&annotation=S4M6WAT2) Â«Overfitting. According to lexico.com (2021), overfitting is â€œThe production of an analysis which corresponds too closely or exactly to a particular set of data, and may therefore fail to fit additional data or predict future observations reliably.â€Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 5](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=6&annotation=79EXZASJ) Â«You overfit when you choose a distribution q(w) that explains the current data very well, but fails to account for the wider range of statistical possibilities encapsulated by p(w)Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 6](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=6&annotation=H4P7BBGZ) Â«The cost of overfitting can therefore be measured by checking how far q(w) diverges from p(w) . The first term of F is a measure of this kind: This term, which is also called relative entropy or Kullback-Leibler divergence, measures how far a distribution q(w) differs from a distribution p(w).3 When q and p are identical, they coincide for every value of the sum. In this case the logarithm is always zero (because log a a = 0 ) so the total value of the sum is zero. As q and p get more and more different, the total value of the term increases. To avoid overfitting, q(w) should be close to p(w).Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 6](zotero://select/library/items/SGFQ77UK)) $\sum_w q(w) \log \frac{q(w)}{p(w)}$

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=6&annotation=2R36ZK3U) Â«Failing to explain the data. Mathematically, â€˜explaining the dataâ€™ means assigning high probability to events w that make the probability of x high. The penalty for failing to explain data is captured by the second term of F:Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 6](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=6&annotation=BTRZCB6C) Â«Variational free energy F is the sum of the penalties for overfitting and failing to explain the data:Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 6](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=6&annotation=T9EWBSFN) Â«w q(w) log q(w) p(w)Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 6](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=6&annotation=PRC3L7X9) Â«w q(w) log 1 p(x w)Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 6](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=6&annotation=D4XPGSIQ) Â«F(p, q, x)= w q(w) log q(w) p(w) Penalty for overfitting + w q(w) log 1 p(x w) Penalty for failing to explain the dataÂ» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 6](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=7&annotation=LGEBIC2P) Â«We set up the inference problem by saying that the agent knows the statistics p(w, x) , but might not have access to the marginal distribution p(x) . The agent was prohibited from following Bayesian PrinciPle for this reason. However, we did not address whether the agent has access to the prior p(w) or the likelihood p(x w). Since F includes both those terms, one would expect the agent needs them in order to use F to guide inference. As it turns out, the agent does not need access to the prior or the likelihood, because (4) simplifies to:Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 7](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=7&annotation=MFGVXGYC) Â«Given our assumptions so far, the agent has access to all three inputs to F in Eq. (5): â€¢ p: A joint distribution over w and x . The agentâ€™s generative model and, in this simple example, also the true general statistical connection between w and x. â€¢ q: A distribution over w . The agentâ€™s credences about the unobservable state, in light of observing a specific piece of data x. â€¢ x : A value of a random variable. The specific piece of data the agent has just observed.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 7](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=7&annotation=UR637T2F) Â«Free energy PrinciPle (inFerence): q(w) â† ï¿½ argmin q F Here argmin q means â€˜choose the distribution q that makes the following term as small as possibleâ€™.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 7](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=7&annotation=HEWSMEA5) Â«Notice that the form of Free energy PrinciPle (inFerence) is the same as that of Bayesian PrinciPle. In both cases you are told to perform a calculation and set q(w) equal to the resulting value. The difference is that Bayesian PrinciPle counsels a direct calculation via Bayesâ€™ theorem. In contrast, Free energy PrinciPle (inFerence) counsels what might be called an indirect calculation. You must assess candidate distributions q in order to find the one that produces the lowest value of F.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 7](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=7&annotation=DHWVDXGN) Â«F(p, q, x)= w q(w) log q(w) p(w, x)Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 7](zotero://select/library/items/SGFQ77UK)) Simplified version of the previous equation [[ĞœĞ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ° Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ° ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑĞ½ĞµÑ€Ğ³Ğ¸Ğ¸]]

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=7&annotation=T5ZPNWRD) Â«n the active inference literature, the penalty for overfitting is often labelled â€˜complexityâ€™. The penalty for failing to explain the data is usually presented as a reward for explaining the data well; it is therefore introduced as the negation of the term we use here, and is called â€˜accuracyâ€™. Consequently, variational free energy is defined as the difference between complexity and accuracy. The goal of inference is described as minimizing complexity while maximizing accuracy. Our presentation is mathematically equivalent.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 7](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=9&annotation=RI9CGPU8) Â«One of the potentially confusing aspects of active inference is that it treats the statistical model p as a measure of both probabilities and preferences at the same time. Later we will discuss possible justifications of this move; for now we assume it is interpretatively valid, in order to give as smooth an exposition as possible.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 9](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=9&annotation=Q4IZNWR9) Â«Recall that Free energy PrinciPle (inFerence) counsels choosing beliefs by minimising a function that measures the cost of inaccuracy. That function, F , is a sum of two kinds of penalty. Action selection is governed in the same way, but with a slightly different cost function called expected free energy and labelled G. The definition of G is closely related to that of F. The interpretation of the two penalty terms changes as the formalism is updated to reflect the fact we are now making measurements over expected future states. Since future states have yet to be observed, the agent must average over them to obtain expected values. The penalties are associated with failing to satisfy preferences and failing to minimize future surprise.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 9](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=9&annotation=QJAANPA5) Â«Failing to satisfy preferences. q(w z) is the assumed distribution over hidden states given our action. If we place the cat in the bedroom, where do we expect it to be? p(w) is now a preference distribution over hidden states. The first penalty term in G is a measure of how far the expected distribution of hidden states diverges from the preference distribution:Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 9](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=9&annotation=RWM5C25L) Â«w q(w z) log q(w z) p(w)Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 9](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=10&annotation=WBMDC728) Â«Failing to minimize future surprise. One of the tenets of active inference is that agents should act to ensure that future data are not too surprising. The second penalty term of G therefore measures how surprising future data would be, on average, if you performed z:Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 10](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=10&annotation=CKYTMBJZ) Â«Overall, expected free energy is a sum of these penalties:Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 10](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=10&annotation=VVDPH8M3) Â«Free energy PrinciPle (action): z â† ï¿½ argmin z GÂ» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 10](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=10&annotation=J4DK8TDT) Â«w q(w z) x p(x w) log 1 p(x w)Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 10](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=10&annotation=VEQYHA9U) Â«G(p, q, z)= w q(w z) log q(w z) p(w) Penalty for failing to satisfy your preferences + w q(w z) x p(x w) log 1 p(x w) Penalty for failing to minimize expected surprise of future dataÂ» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 10](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=11&annotation=WBYMZQJT) Â«There are at least two reasons why this interpretation should be distinguished from utilities as decision theory traditionally understands them. First, you should not simply prefer that your cat always be the temperature that happens to occur most often according to the frequency distribution. Healthy functioning entails some fluctuation of temperatures throughout the day. The goal is not to maximise the value of this distribution, but to match future event frequencies to it. Second, preferences are just one consideration that must be taken into account when choosing actions. The preference penalty must be balanced against the surprise penalty.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 11](zotero://select/library/items/SGFQ77UK)) [[ĞœĞ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ° Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ° ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑĞ½ĞµÑ€Ğ³Ğ¸Ğ¸]]

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=12&annotation=M7XXRQ3Y) Â«One of the ways proponents of the framework turn this unusual interpretation to their advantage is by casting action as a form of inference: The mechanism underlying [minimizing expected free energy] is formally symmetric to perceptual inference, i.e., rather than inferring the cause of sensory data an organism must infer actions that best make sensory data accord with an internal representation of the environment. Buckley etÂ al. (2017: p.57), emphasis addedÂ» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 12](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=12&annotation=LI4XSBI5) Â«By starting with an inference problem in the form of expected free energy minimization, preferences emerge as the first term of Eq. (8). But attempting to achieve these preferences must be balanced against the second term, which explicitly counsels minimizing future surprise.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 12](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=13&annotation=7V5NXJYG) Â«(Pearl 1988). Roughly, in Pearlâ€™s sense the â€˜Markov blanketâ€™ of a focal node is the set of nodes that provide total information about the focal node. However, Markov blankets have taken on a special usage within active inference (Bruineberg etÂ al. 2021). In the sense required here, a Markov blanket can be understood as the set of nodes that â€˜screen offâ€™ the agent from nodes considered external to it.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 13](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=14&annotation=ES2JXETP) Â«The correspondence between high values of F and life-threatening states leads to a third form of the free energy principle: Free energy PrinciPle (selection): any system that survives long enough will act so as to appear to be minimizing F.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 14](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=14&annotation=W8EYYCDR) Â«In recent work Friston gives a deflationary interpretation on which agents do not in fact minimize anything, but perform acts which can be interpreted as minimizing F.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 14](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=15&annotation=QFTC55I6) Â«Free energy PrinciPle (selection) interprets p as a kind of fitness function in the form of a probability distribution over sensory states. When we measured the temperature of our cat, we obtained a frequency distribution that acted both as a description of what happened when the cat was previously healthy and as a prescription of what temperatures the cat should have if we want it to remain healthy. Free energy PrinciPle (selection) expands the scope of this basic idea, from cats to every biological system, and from temperature to every measurable property.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 15](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=16&annotation=22XBCL9N) Â«energy PrinciPle (selection) captures the rather banal point that systems can only ever occupy survivable states. If you are likely to be in states your successful ancestors were in, then you are likely to be successful. This trivial observation is reflected mathematically by the fact that variational free energy contains a reciprocal of p(w, x) : high values of p(w, x) therefore produce low values of F . Indeed, any function that contains this reciprocal (or its logarithm) as a component will be infinite when the probability is zero.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 16](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=16&annotation=BPTHXMC6) Â«It is easy to show that variational free energy is an upper bound on surprisal.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 16](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=16&annotation=RST29NQ2) Â«roof: rearranging (4) gives F = âˆ‘ w q(w) log q(w) p(wï¿½x) + log 1 p(x) . The first term is a relative entropy, which by Jensenâ€™s inequality is always greater than or equal to zero (Cover and Thomas 2006,Â p. 28). Therefore F â‰¥ log 1 p(x) .Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 16](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=17&annotation=92R2Z3CQ) Â«minimizing variational free energy is not the only way to minimize surprise. Any non-negative function added to surprisal is an upper bound on surprisal. Proponents need another premise that singles out variational free energy as the function organisms should be treated as minimizing.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 17](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=17&annotation=4DM8MTX9) Â«In our first model, p was a generative model employed by an agent. It was therefore interpreted as representing probabilities. 2. In our second model, in addition to representing probabilities, p measured the desirability of certain future states over others. It was therefore interpreted as representing preferences. 3. In our third model, p tallied the historical frequencies of a set of (hypothetical) ancestors. It was therefore interpreted as representing the fitness of different states.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 17](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=17&annotation=ZU7MWH6N) Â«Supporters of the framework often point to the third role to explain how p can simultaneously fulfil the first two. A historical tally of successful states denotes probabilities (i.e. ancestral frequencies) and preferences (i.e. future expected fitness).Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 17](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=17&annotation=BRRV8P4S) Â«Entities that employ representations to act successfully are distinct in important ways from entities that can be treated as if they employ representations as a consequence of the effects of physical laws.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 17](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=17&annotation=47FGBK98) Â«Sprevak (2020, Sect.Â 6.3) convincingly argues that Friston invokes two distinct senses of free energy, which here correspond roughly to roles 1 and 3. Sprevak cites Colombo and Wright (2018) as drawing a similar distinction. Williams (2021) distinguishes descriptive and explanatory versions of the free energy principle, seemingly tracking the same issue.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 17](zotero://select/library/items/SGFQ77UK)) > Friston uses the concept of free energy in at least two senses: probabilities and fitness function.

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=18&annotation=CTNZYSGW) Â«We also assumed that there was a single cause, w , of sensory data. Realistically, the external world is a panoply of criss-crossing causal paths. An adequate generative model would contain terms representing at least some of the interactions between unobservable states. Active inference captures these features by treating agents as employing hierarchical models of their external worlds. The first level of the hierarchy x is the sensory data, the second level w1 represents whatever causes sensory data, the third level w2 represents whatever causes w1 , and so on.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 18](zotero://select/library/items/SGFQ77UK)) > Hierarchical evolutionary model of convention formation? â€” [[@hawkins2021]] + https://probmods.org/

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=18&annotation=XEPUYPWN) Â«In principle, agents could be uncertain about any aspect of their representation of the world, so every model component can be subject to updating in light of evidence.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 18](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=18&annotation=7Z6C9BVZ) Â«Regarding action, instead of a single act z the framework enables decisions about sequences of acts. Such sequences are called policies and are usually labelled ğœ‹. Expected free energy can be calculated across an entire policy in order to determine which sequence of acts is optimal.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 18](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=20&annotation=CM85F96P) Â«most successful neural models were perhaps those spawned by the predictive processing tradition. Predictive processing was inspired by predictive coding, a technique in communications engineering (Elias 1955). In the 1980s and 1990s neuroscientists began investigating its plausibility as a model of visual perception (Kawato etÂ al. 1993; Rao and Ballard 1999; Srinivasan etÂ al. 1982). In the early 2000s, Friston (2002,Â p. 131) claimed that a predictive processing system could be constructed that performs variational inference (see also Friston 2003,Â pp. 1339â€“1340).Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 20](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=20&annotation=43FC5BJQ) Â«Very roughly, we can understand the relationship between these aspects in terms of Marrâ€™s hierarchy, which is usually said to have three levels: computational, algorithmic, and implementational (Marr 1982). In Fristonâ€™s scientific model of predictive processing, the computation is variational inference. The algorithm is the expectation-maximisation algorithm, a two-step process whereby two different mathematical operations are performed iteratively. Neal and Hinton (1998) had already shown that a version of that algorithm minimizes variational free energy. Friston claimed the algorithm could be implemented by the activities of (and structural relations between) individual neurons (for a simplified example see Bogacz 2017, Sect.Â 2â€“3).Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 20](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=23&annotation=ZE9S9GMM) Â«he claim that a small neural network is capable of minimizing variational free energy via encoding prediction error is verifiable by actually building such a network, as Bogacz (2017, Sect.Â 2â€“3) shows.Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 23](zotero://select/library/items/SGFQ77UK))

[Go to annotation](zotero://open-pdf/library/items/FHLDUYF5?page=30&annotation=KZ29JV65) Â«The FEP is a mathematical formulation that explains, from first principles, the characteristics of biological systems that are able to resist decay and persist over time. It rests on the idea that all biological systems instantiate a hierarchical generative model of the world that implicitly minimises its internal entropy by minimising free energy. Ramstead etÂ al. (2018,Â p. 2)Â» ([Mann Ğ¸ Ğ´Ñ€., 2022, p. 30](zotero://select/library/items/SGFQ77UK))